{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import LlamaTokenizer\n",
    "from icecream import ic\n",
    "from config.config import conf\n",
    "from model.Transformer import Transformer\n",
    "import torch.utils.data as Data\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "import shutil\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from ordered_set import OrderedSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### norm ###############################################\n",
    "class LayerNorm(nn.Module):\n",
    "    '''layer normalization\n",
    "\n",
    "    Args:\n",
    "        shape (int): length of the embedding vector\n",
    "        eps (float): a small number to prevent division by zero\n",
    "    '''\n",
    "    def __init__(self, shape, eps = 1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # initialize two learnable parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # prevent division by zero\n",
    "        self.eps = eps\n",
    "        \n",
    "    # 8 * len * d_model\n",
    "    def forward(self, x):\n",
    "        # mean and variance\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased = False, keepdim=True)\n",
    "        normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * normalized + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### attention ###############################################\n",
    "class SelfAttention(nn.Module):\n",
    "    '''self attention by dot-product\n",
    "\n",
    "    Args:\n",
    "        scale_factor (): scale_factor\n",
    "        dropout (float): dropout rate\n",
    "    '''\n",
    "    def __init__(self, scale_factor, dropout=0.0):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "    # (4d_k + 3)len*len (should multiply by heads)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # matmul & scale\n",
    "        # 2*d_k * len * len + len*len\n",
    "        scores = torch.matmul(q, k.transpose(2, 3)) / self.scale_factor\n",
    " \n",
    "        # optional mask\n",
    "        # 2*len*len\n",
    "        if mask is not None:\n",
    "            # use -1e9 as the large negative value to mask the padding tokens\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        # softmax\n",
    "        # 3 * len * len\n",
    "        scores = torch.softmax(scores, dim=-1)\n",
    "        # matmul\n",
    "        # 2*d_k * len * len\n",
    "        output = torch.matmul(scores, v)\n",
    "        # 返回 output和注意力分数\n",
    "        return output, scores\n",
    "    \n",
    "\n",
    "class MultiAttention(nn.Module):\n",
    "    '''multi-head attention\n",
    "\n",
    "    Args:\n",
    "        n_heads (int): number of heads\n",
    "        dim (int): length of the embedding vector\n",
    "        dim_k (int): dim of k\n",
    "        dim_v (int): dim of v\n",
    "        dropout (float): dropout rate\n",
    "    '''\n",
    "    def __init__(self, n_heads = 8, dim = 512, dim_k = 64, dim_v = 64,  dropout=0.1):\n",
    "        super(MultiAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    " \n",
    "        # weight matrices for Q, K, V\n",
    "        # the linear layer represents the weight matrices\n",
    "        self.Wq = nn.Linear(dim, n_heads * dim_k, bias=False)\n",
    "        self.Wk = nn.Linear(dim, n_heads * dim_k, bias=False)\n",
    "        self.Wv = nn.Linear(dim, n_heads * dim_v, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * dim_v, dim, bias=False)\n",
    " \n",
    "        self.attention = SelfAttention(scale_factor=dim_k ** 0.5)\n",
    " \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = LayerNorm(dim, eps=1e-6)\n",
    " \n",
    "    # 8 * len * d_model * d_model + 4 * len * len * d_model + 3 * len * len * h + 9 * len * d_model\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v：[batch_size, seq_num, dim]\n",
    "        # len_k为输入的序列长度\n",
    "        batch_size = q.size(0)\n",
    "        # for residual connection\n",
    "        residual = q\n",
    "        # breakpoint()\n",
    " \n",
    "        # multiplied by W^Q, W^K, W^V\n",
    "        # (batch_size, length, n_heads, dim_k) => (batch_size, n_heads, length, dim_k)\n",
    "        # 3 * 2 * len * d_model * d_model\n",
    "        query = self.Wq(q).view(batch_size, -1, self.n_heads, self.dim_k).transpose(1, 2)\n",
    "        key   = self.Wk(k).view(batch_size, -1, self.n_heads, self.dim_k).transpose(1, 2)\n",
    "        value = self.Wv(v).view(batch_size, -1, self.n_heads, self.dim_v).transpose(1, 2)\n",
    " \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) \n",
    "        \n",
    "        # (4d_k + 3) * len * len * h => \n",
    "        # 4 * d_model * len * len + 3 * len * len * h\n",
    "        x, attn = self.attention(query, key, value, mask=mask)\n",
    " \n",
    "        # Transpose to move the head dimension back: b x lq x n x dv\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "        # (batch_size, n_heads, length, d_k/d_v) => (batch_size, length, n_heads, d_k/d_v) => (batch_size, length, dim)\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.dim_k)\n",
    "        # the final linear layer\n",
    "        # 2 * len * d_model * d_model\n",
    "        # > we apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\n",
    "        x = self.dropout(self.fc(x))\n",
    "        # add in add & norm\n",
    "        # len * d_model\n",
    "        x += residual\n",
    "        # norm in add & norm\n",
    "        # 8 * len * d_model\n",
    "        x = self.layer_norm(x)\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### embedding & encoding ###############################################\n",
    "class Embeddings(nn.Module):\n",
    "    '''word embeddings\n",
    "\n",
    "    Args:\n",
    "        dim (int): length of the embedding vector\n",
    "        vocab (int): number of words in the vocabulary\n",
    "    '''\n",
    "    def __init__(self, dim, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        # 调用nn.Embedding预定义层，获得实例化词嵌入对象self.lut\n",
    "        self.lut = nn.Embedding(vocab, dim)\n",
    "        self.dim = dim  #表示词向量维度\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.dim)\n",
    "    \n",
    "class RoPE(nn.Module):\n",
    "    '''positional encoding with sin and cos\n",
    "\n",
    "    Args:\n",
    "        dim (int): length of the embedding vector, same as embedding dim\n",
    "        max_len (int): max length of the input sequence\n",
    "        dropout (float): dropout rate\n",
    "    '''\n",
    "    def __init__(self, dim, max_len=5000, dropout=0.1):\n",
    "        super(RoPE, self).__init__()\n",
    " \n",
    "        # positional encoding matrix\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)   \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)   \n",
    "        # batch size occupies one dim\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, dim] embedding\n",
    "        x = x + self.pe[:, :x.size(1)].clone().detach() \n",
    "        # > We apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### ff ###############################################\n",
    "class FeedForward(nn.Module):\n",
    "    '''position-wise feed-forward network\n",
    "\n",
    "    Args:\n",
    "        dim (int): dimension of input/output\n",
    "        hidden_dim (int): dimension of hidden layer\n",
    "        dropout (float): dropout rate\n",
    "    '''\n",
    "    def __init__(self, dim = 512, hidden_dim = 2048, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim, bias=False),\n",
    "            nn.ReLU(),\n",
    "            # > we apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\n",
    "            nn.Linear(hidden_dim, dim, bias=False),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.layer_norm = LayerNorm(dim, eps=1e-6)\n",
    "        \n",
    "    # 4 * len*d_model * d_ff + 9 * len * d_model\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, len, dim]\n",
    "        residual = x\n",
    "        x = self.linear(x)\n",
    "        # add in add & norm\n",
    "        x += residual\n",
    "        # norm in add &norm\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### decoder ###############################################\n",
    "# mask out previous tokens by a triangular matrix\n",
    "# position of previous tokens will be True\n",
    "def mask_subsequence(sequence):\n",
    "    # get the size of the sequence\n",
    "    batch_size, seq_len = sequence.size()\n",
    "    # create a triangular matrix\n",
    "    mask = torch.tril(torch.ones(batch_size, seq_len, seq_len)).bool()\n",
    "    # mask = torch.triu(torch.ones(batch_size, seq_len, seq_len), diagonal=1).bool()\n",
    "    return mask\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    '''one layer of decoder\n",
    "    \n",
    "    Args:\n",
    "        dim (int): length of the embedding vector\n",
    "        ff_dim (int): length of the hidden layer in the feedforward network\n",
    "        n_heads (int): number of heads\n",
    "    '''\n",
    "    def __init__(self, dim, ff_dim, n_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.decoder_masked_atten = MultiAttention(dim=dim, n_heads=n_heads, dim_k=dim//n_heads, dim_v=dim//n_heads)\n",
    "        self.ffnet = FeedForward(dim=dim, hidden_dim=ff_dim)\n",
    "    \n",
    "    # inputs is the input of the decoder\n",
    "    # masked_atten_mask is the mask for the masked multi-head attention\n",
    "    def forward(self, inputs, masked_atten_mask):\n",
    "        outputs, masked_attention = self.decoder_masked_atten(inputs, inputs, inputs, masked_atten_mask)\n",
    "        # the results from last masked multi-head attention is used as Q\n",
    "        outputs = self.ffnet(outputs)\n",
    "        return outputs, masked_attention\n",
    "    \n",
    "# for encoder-decoder architecture\n",
    "class Decoder(nn.Module):\n",
    "    '''decoder itself\n",
    "\n",
    "    Args:\n",
    "        vocab (int): number of words in the vocabulary\n",
    "        emb_dim (int): length of the embedding vector\n",
    "        ff_dim (int): length of the hidden layer in the feedforward network\n",
    "        context_len (int): length of the context\n",
    "        n_layers (int): number of layers\n",
    "        n_heads (int): number of heads\n",
    "        device\n",
    "    '''\n",
    "    def __init__(self, vocab, emb_dim, ff_dim, context_len, n_layers = 6, n_heads = 8, device = 'cuda'):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embeddings(emb_dim, vocab)\n",
    "        self.encoding = RoPE(emb_dim)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(emb_dim, ff_dim, n_heads) for _ in range(n_layers)])\n",
    "        self.device = device\n",
    "       \n",
    "    # inputs is the input of the decoder\n",
    "    def forward(self, inputs): \n",
    "        # mask out stop words and subsequence in the inputs\n",
    "        masked_atten_mask = mask_subsequence(inputs).to(self.device)\n",
    "        # embedding & encoding\n",
    "        embedding = self.embedding(inputs)\n",
    "        outputs = self.encoding(embedding)\n",
    "        # decode\n",
    "        for layer in self.layers:\n",
    "            outputs, _ = layer(outputs, masked_atten_mask)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### transformer ###############################################\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, emb_dim = 512, ff_dim=2048, context_len=256,\n",
    "                 dec_layers = 6, n_heads = 8,\n",
    "                 vocab = 1e4, device = 'cuda'):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.decoder = Decoder(vocab, emb_dim, ff_dim, context_len, dec_layers, n_heads, device)\n",
    "        self.projection = nn.Linear(emb_dim, vocab, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.context_len = context_len\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        dec_outputs = self.decoder(inputs)\n",
    "        # softmax is included in CrossEntropyLoss\n",
    "        # 2 * d_model * len * vocab\n",
    "        outputs = self.projection(dec_outputs)\n",
    "        return outputs.view(-1, outputs.size(-1))\n",
    "    \n",
    "    def generate(self, context, max_len = 200, terminate = None):\n",
    "        for _ in range(max_len):\n",
    "            dec_outputs = self.decoder(context[:, -self.context_len:])\n",
    "            outputs = self.projection(dec_outputs)\n",
    "            outputs = outputs[:, -1, :]\n",
    "            outputs = self.softmax(outputs)\n",
    "            # sample one token id as the next\n",
    "            next_token = torch.multinomial(outputs, 1)\n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "            # terminiate if the token is the end token\n",
    "            if terminate is not None and next_token.item() == terminate:\n",
    "                break\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admini/anaconda3/envs/tsf/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2004: FutureWarning: Calling LlamaTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7986 tokens in total\n",
      "0.368633 M input tokens in total\n",
      "33.359872 M parameters\n",
      "29.27104 M non-embedding parameters\n",
      "\n",
      " CAMILLO:\n",
      "Come, come; 'tis a second to me:\n",
      "I have no hat you know, sir:\n",
      "You do not wedged, I have underta'enLoved\n",
      "His regiment lies half a mile at least\n",
      "South from the mighty power of the king.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# wget -c https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# git clone https://github.com/facebookresearch/llama\n",
    "# apply as the guidance in the README.md\n",
    "# bash llama/download.sh\n",
    "\n",
    "# https://github.com/bl0nder/makespeare/blob/main/makespeare.py\n",
    "def tokenize(path: str = \"input.txt\"):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"tokenizer.model\")\n",
    "    # padding and beginning of sequence\n",
    "    # special_tokens = ['<s>']\n",
    "    special_tokens = ['<s>', '</s>']\n",
    "    # '<unk>': 0  '<s>': 1  '</s>': 2  '<0x0A>': 13(carriage return)\n",
    "    \n",
    "    # convert_tokens_to_string\n",
    "    # (Pdb) tokenizer('<s>cdc</s>')\n",
    "    # {'input_ids': [1, 1, 274, 13891, 2], 'attention_mask': [1, 1, 1, 1, 1]}\n",
    "    # (Pdb) tokenizer('<s>cdc</s>', add_special_tokens=False)\n",
    "    # {'input_ids': [1, 274, 13891, 2], 'attention_mask': [1, 1, 1, 1]}\n",
    "\n",
    "    # open input.txt and tokenize it\n",
    "    with open(path, \"r\") as f:\n",
    "        text = f.read()\n",
    "        raw_tokens = tokenizer.tokenize(text)\n",
    "    # to ensure the set is in the same order\n",
    "    tokens = OrderedSet(raw_tokens)\n",
    "    for tk in special_tokens: \n",
    "        tokens.discard(tk)\n",
    "    vocab = {}\n",
    "    reverse_vocab = {}\n",
    "    for i, tk in enumerate(special_tokens):\n",
    "        vocab[tk] = i\n",
    "        reverse_vocab[i] = tk\n",
    "    for id, token in enumerate(tokens):\n",
    "        # leave 0 for padding\n",
    "        vocab[token] = id + len(special_tokens)\n",
    "        reverse_vocab[id + len(special_tokens)] = token\n",
    "    raw_tokens.insert(0, '<s>')\n",
    "    # this kind of data does not need this kind of separation\n",
    "    i = 0\n",
    "    while i < len(raw_tokens) - 1:\n",
    "        if raw_tokens[i] == '<0x0A>' and raw_tokens[i + 1] == '<0x0A>':\n",
    "            # indicate the beginning of a new sequence\n",
    "            raw_tokens[i] = '</s>'\n",
    "            raw_tokens[i + 1] = '<s>'\n",
    "            i += 1\n",
    "        i += 1\n",
    "    if raw_tokens[-1] == '<s>':\n",
    "        # raw_tokens[-1] = '<0x0A>'\n",
    "        raw_tokens.pop()\n",
    "        \n",
    "    id_text = torch.LongTensor([vocab[token] for token in raw_tokens])\n",
    "    print(f'{len(vocab)} tokens in total')\n",
    "    print(f'{len(id_text) /1e6} M input tokens in total')\n",
    "            \n",
    "    return vocab, reverse_vocab, id_text\n",
    "\n",
    "# decode a list of ids to a readable string\n",
    "def decode(ids, reverse_vocab):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"tokenizer.model\", add_prefix_space=False)\n",
    "    res = ''.join([reverse_vocab[id.item()] for id in ids[0]])\n",
    "    res = tokenizer.convert_tokens_to_string(res)\n",
    "    res = res.replace('<0x0A>', '\\n')\n",
    "    res = res.replace('</s>', '\\n')\n",
    "    res = res.replace('<s>', '\\n')\n",
    "    return res\n",
    "\n",
    "class TinyDataset(Data.Dataset):\n",
    "    def __init__(self, inputs, context_length, pad_idx = 0):\n",
    "        super(TinyDataset, self).__init__()\n",
    "        self.inputs = inputs\n",
    "        self.context_length = context_length\n",
    "        self.pad_idx = pad_idx\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.inputs.size(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.inputs[idx: idx+self.context_length]\n",
    "        y = self.inputs[idx+1: idx+self.context_length+1]\n",
    "        # padding\n",
    "        if x.size(0) < self.context_length:\n",
    "            x = torch.cat([x, torch.full([self.context_length - x.size(0)], self.pad_idx, dtype=torch.long)])\n",
    "        if y.size(0) < self.context_length:\n",
    "            y = torch.cat([y, torch.full([self.context_length - y.size(0)], self.pad_idx, dtype=torch.long)])\n",
    "            # y = torch.cat([y, torch.zeros(self.context_length - y.size(0), dtype=torch.long)])\n",
    "        return x, y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    vocab, reverse_vocab, id_text = tokenize()\n",
    "    split_idx = int(len(id_text) * 0.9)\n",
    "    train_data = id_text[:split_idx]\n",
    "    val_data = id_text[split_idx:]\n",
    "    \n",
    "    model = Transformer(emb_dim=conf['emb_dim'], ff_dim=conf['ff_dim'], context_len=conf['context_length'],\n",
    "                        dec_layers=conf['decoder_layers'], n_heads=conf['heads'],\n",
    "                        vocab=len(vocab), device=device).to(device)\n",
    "    print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "    print((sum(p.numel() for p in model.parameters()) - conf['emb_dim'] * len(vocab))/1e6, 'M non-embedding parameters')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # 5e-5\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=conf['lr'])\n",
    "    # 1e-3\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=conf['lr'], momentum=0.99)\n",
    "    \n",
    "    train_loader = Data.DataLoader(TinyDataset(train_data, conf['context_length'], vocab['<0x0A>']), conf['batch_size'], True)\n",
    "    valid_loader = Data.DataLoader(TinyDataset(val_data, conf['context_length'], vocab['<0x0A>']), conf['batch_size'], True)\n",
    "    os.makedirs(os.path.join(conf['ckpt_path'], conf['exp']), exist_ok=True)\n",
    "    # print('Start training...')\n",
    "    # shutil.copyfile('config/config.py', os.path.join(conf['ckpt_path'], conf['exp'], 'config.py'))\n",
    "    # writer = SummaryWriter(os.path.join('logs', conf['exp']))\n",
    "    # for idx in trange(1, int(conf['iterations']) + 1):\n",
    "    #     inputs, targets = next(iter(train_loader))\n",
    "    #     inputs, targets = inputs.to(device), targets.to(device)\n",
    "    #     optimizer.zero_grad()\n",
    "    #     outputs = model(inputs)\n",
    "    #     loss = criterion(outputs, targets.view(-1))\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "        \n",
    "    #     # lr decay\n",
    "    #     decay_steps = conf['lr_decay'] * 1000\n",
    "    #     decay_factor = 0.1 ** (1 / decay_steps)\n",
    "    #     if idx > int(conf['iterations'] * conf['decay_initiation']):\n",
    "    #         for param_group in optimizer.param_groups:\n",
    "    #             param_group['lr'] = param_group['lr'] * decay_factor\n",
    "            \n",
    "    #     if idx % (int(conf['log_iter']) / 10) == 0:\n",
    "    #         writer.add_scalar('loss/loss', loss, idx)\n",
    "    #         writer.add_scalar('lr', optimizer.param_groups[0]['lr'], idx)\n",
    "            \n",
    "    #     if idx % int(conf['log_iter']) == 0:\n",
    "    #         val_loss = 0\n",
    "    #         with torch.no_grad():\n",
    "    #             model.eval()\n",
    "    #             for i in range(int(conf['val_iterations'])):\n",
    "    #                 x, y = next(iter(valid_loader))\n",
    "    #                 x, y = x.to(device), y.to(device)\n",
    "    #                 outputs = model(x)\n",
    "    #                 val_loss += criterion(outputs, y.view(-1))\n",
    "    #             writer.add_scalar('loss/val_loss', val_loss / conf['val_iterations'], idx)\n",
    "    #             model.train()\n",
    "    #         tqdm.write(f'Iteration: {idx} loss = {loss:.8f} val_loss = {val_loss / conf[\"val_iterations\"]:.8f}')\n",
    "            \n",
    "    #     if idx % int(conf['ckpt_iter']) == 0:\n",
    "    #         torch.save(model.state_dict(), os.path.join(conf['ckpt_path'], conf['exp'], f'model_{idx}.pt'))\n",
    "      \n",
    "    # writer.close()\n",
    "    # print('Done!')\n",
    "    \n",
    "    # generate\n",
    "    ckpt = torch.load(os.path.join(conf['ckpt_path'], conf['exp'], f'model_30000.pt'))\n",
    "    # ckpt = torch.load(os.path.join(conf['ckpt_path'], conf['exp'], f'model_{int(conf[\"iterations\"])}.pt'))\n",
    "    model.load_state_dict(ckpt)\n",
    "    # randomly pick one word as the initial input\n",
    "    model.eval()\n",
    "    inputs = torch.LongTensor([[vocab['<s>'], vocab['▁C'], vocab['AM'], vocab['ILL'], vocab['O'], vocab[':']]]).to(device)\n",
    "    result = model.generate(inputs, 64, vocab['</s>'])\n",
    "    res = decode(result, reverse_vocab)\n",
    "    print(res)\n",
    "    with open(os.path.join(conf['ckpt_path'], conf['exp'], f'output_{int(conf[\"iterations\"])}.txt'), 'a+') as f:\n",
    "        f.write(res)\n",
    "        f.write('\\n----------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 301142), started 0:01:24 ago. (Use '!kill 301142' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f75c5d6a414f5718\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f75c5d6a414f5718\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters & FLOP\n",
    "\n",
    "## Parameter\n",
    "\n",
    "Notation that will be used: \n",
    "\n",
    "|   symbol    |                           meaning                            |\n",
    "| :---------: | :----------------------------------------------------------: |\n",
    "|     $V$     |                      size of vocabulary                      |\n",
    "|     $N$     | number of encoder layers and decoder layers (assumed to be the same here for simplicity) |\n",
    "| $d_{model}$ |                length of the embedding vector                |\n",
    "|  $d_{ff}$   | size of the hidden layer of the feedforward network, usually $4d_{model}$ |\n",
    "|     $h$     |       number of the heads used in multi-head attention       |\n",
    "|    $d_k$    | length of query vectors and key vectors, usually $h\\cdot d_k = d_{model}$ |\n",
    "|    $d_v$    |          length of value vectors, usually $d_v=d_k$          |\n",
    "\n",
    "Firstly, we adopt the same hyper-parameters as the base model of the transformer model, and that is\n",
    "\n",
    "- $d_{model}=512$\n",
    "- $d_{ff}=2048$\n",
    "- $h=8$\n",
    "- $d_k=d_v=64$\n",
    "- $V=7986\\approx8000$ (given by llama2 tokenizer on [tiny Shakespeare dataset](https://huggingface.co/datasets/tiny_shakespeare))\n",
    "\n",
    "We use the decoder-only transformer model. Theoretically we can have\n",
    "\n",
    "- parameters of embeddings: $P_{emb}=Vd_{model}$\n",
    "- parameters of one layer normalization: $P_{layernorm}=2d_{model}$\n",
    "- parameters of multi-head attention: $P_{multi}=2hd_kd_{model}+2hd_vd_{model}+P_{layernorm}$\n",
    "- parameters of the feedforward network: $P_{ff}=2d_{model}d_{ff}+P_{layernorm}=8d_{model}^2+P_{layernorm}$\n",
    "- parameters of one decoder layer: $P_{declayer}=P_{multi}+P_{ff}$\n",
    "- parameters of the projection layer: $P_{proj}=Vd_{model}$\n",
    "- parameters of the non-embeddings: $P_{model}=NP_{declayer}+P_{proj}=N(12d_{model}^2+4d_{model})+Vd_{model}$\n",
    "- parameters of the full model with embeddings: $P_{full}=P_{emb}+NP_{declayer}+P_{proj}=N(12d_{model}^2+4d_{model})+2Vd_{model}$\n",
    "\n",
    "With the equation, we can tell that theoretically the modal has **22974976** parameters. \n",
    "\n",
    "Experimentally, we can calculate the number of parameters with code like `sum(p.numel() for p in model.parameters())`, and we will obtain the same number **22974976**. \n",
    "\n",
    "\n",
    "\n",
    "## FLOP\n",
    "\n",
    "Before calculation, some extra notations need to be demonstrated:\n",
    "\n",
    "|   symbol    |                         meaning                          |\n",
    "| :---------: | :------------------------------------------------------: |\n",
    "|    $N_b$    |               batch size used in training                |\n",
    "|  $d_{ctx}$  |                    length of context                     |\n",
    "| $d_{token}$ | how many tokens used in training, $d_{token}=d_{ctx}N_b$ |\n",
    "|    $N_i$    |               total iterations of training               |\n",
    "\n",
    "Firstly, we should obtain the FLOPs of the used GPU. With the `deviceQuery` tool from cuda, we can know that RTX 4090 has 16384 cores and a max clock rate of 2.6 GHz. Then we can approximately calculate the computation capability by $2 * 2.6 * 16384=85196.8\\ GFLOPs\\approx 85TFLOPs$. And there is another way to justify the FLOPs, that is to check out [the whitepaper published by NVIDIA](https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf). From the whitepaper we can know the official number of RTX 4090 is **82.6 TFLOPs**, which is quite similar to that calculated by hand. So we can confirm that the GPU can perform 82.6 TFLOP per second. \n",
    "\n",
    "Let's calculate the FLOP of the decoder-only transformer model. Theoretically we have \n",
    "\n",
    "- FLOP of embedding and encoding: $C_{emb}=d_{token}d_{model}+d_{token}d_{model}=2d_{token}d_{model}$\n",
    "- FLOP of one layer normalization on input of $(d_{token}, d_{model})$: $C_{layernorm}=8d_{token}d_{model}$\n",
    "- FLOP of softmax on input of $(d_{token}, d_{model})$: $C_{softmax}=3d_{token}d_{model}$\n",
    "- FLOP of self attention: $C_{self}=2d_kd_{token}^2+d_{token}^2+2d_{token}^2+3d_{token}^2+2d_kd_{token}^2=(4d_k+3)d_{token}^2$\n",
    "- FLOP of multi-head attention: $C_{multi}=3*2d_{token}d_{model}^2+hC_{self}+2d_{token}d_{model}^2+d_{token}d_{model}+C_{layernorm}$\n",
    "- FLOP of the feedforward network on input of $(d_{token}, d_{model})$: $C_{ff}=4d_{token}d_{model}d_{ff}+d_{token}d_{model}+C_{layernorm}=16d_{token}d_{model}^2+9d_{token}d_{model}$\n",
    "- FLOP of one decoder layer: $C_{declayer}=C_{ff}+C_{multi}$\n",
    "- FLOP of the projection layer: $C_{proj}=2Vd_{token}d_{model}$\n",
    "- FLOP of the model per token: $C=(C_{emb}+NC_{declayer}+C_{proj})/d_{token}\\approx N(24d_{model}^2+18d_{model})+2Vd_{model}$\n",
    "\n",
    "From the equations above, we can tell that as $N$ and $d_{model}$ increase, $C$ and $2P_{model}$ would become closer and closer, and we will obtain the famous scaling law $C=2P$. \n",
    "\n",
    "We specify all the hyper-parameters and calculate expected FLOP specifically with \n",
    "\n",
    "- $d_{ctx}=256$\n",
    "- $N_b=32$\n",
    "- $N_i=100000$\n",
    "\n",
    "The expected FLOP per token would be **202.76 M**. Further we can have the expected total FLOP to be **498308 TFLOP** (3 times the calculation result including forward and backward), and the expected training time would be around **6076 seconds**. \n",
    "\n",
    "Experimentally, we can simply record the training used with the specified hyper-parameters, and the total training time is around **6000 seconds**. That is quite acceptable.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Firstly we need to process the dataset. The tokenizer of llama2 includes some special tokens, like `<s>` specifying the beginning of the sequence and `</s>` specifying the ending of the sequence. Although the problem description askes us to generate *poems* of Shakespeare, but in fact the input data consists of multiple *plays* rather than *poems*. I added `<s>` and `</s>` to mark each dialogue as one sequence. \n",
    "\n",
    "Then I trained the model with the hyper-parameters like\n",
    "\n",
    "```python\n",
    "conf = {\n",
    "    'exp': 'base',\n",
    "    # length of embedding vector\n",
    "    'emb_dim': 512,\n",
    "    'ff_dim': 512*4,\n",
    "    'heads': 8,\n",
    "    'decoder_layers': 8,\n",
    "    # how many tokens processed at a time\n",
    "    'context_length': 256,\n",
    "    'batch_size': 32,\n",
    "    'iterations': 8e4,\n",
    "}\n",
    "```\n",
    "\n",
    "The input data contains **0.36** tokens in total but the model has **29 M** non-embedding parameters. So we can clearly observe over-fitting. The validation loss becomes larger and larger. \n",
    "\n",
    "So a smaller model is necessary. I tried another model like \n",
    "\n",
    "```python\n",
    "conf = {\n",
    "    'exp': 'trial2',\n",
    "    # length of embedding vector\n",
    "    'emb_dim': 24,\n",
    "    'ff_dim': 24*4,\n",
    "    'heads': 8,\n",
    "    'decoder_layers': 6,\n",
    "    # how many tokens processed at a time\n",
    "    'context_length': 256,\n",
    "    'batch_size': 32,\n",
    "    'iterations': 6e4,\n",
    "    \n",
    "}\n",
    "```\n",
    "\n",
    "And the over-fitting becomes weaker. The loss curve becomes more beautiful too. \n",
    "\n",
    "From base model: \n",
    "\n",
    "```\n",
    " CAMILLO:\n",
    "Do you blanks too:\n",
    "You know the just cause remove.\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    " CAMILLO:\n",
    "I would not stay awhile.\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    " CAMILLO:\n",
    "Well, my lord.\n",
    "\n",
    "----------------------------------------------\n",
    "```\n",
    "\n",
    "From biggerlr model:\n",
    "\n",
    "```\n",
    " CAMILLO:\n",
    "O, not depart to Warwick, how long by.\n",
    "I'I twice, be great: hold you be still: knock and threats\n",
    "Great-board, from his majesty as flower of words,\n",
    "dy, but thought of how o' hence and loathed the rottench\n",
    "----------------------------------------------\n",
    "\n",
    " CAMILLO:\n",
    "I am a little world is sweet boy.\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    " CAMILLO:\n",
    "Are you well; it?\n",
    "\n",
    "----------------------------------------------\n",
    "```\n",
    "\n",
    "From smallerlr model:\n",
    "\n",
    "```\n",
    " CAMILLO:\n",
    "She be made flatter? I know for my bottomister from Here kind\n",
    "Ege,\n",
    "That a disposition she lay that I saw my table North by the Duke:\n",
    "Hounds, or thy father\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    " CAMILLO:\n",
    "Ineless hour serviceured Edward.\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    " CAMILLO:\n",
    "My son, you to:\n",
    "The ranksithmetic makes the green; Lord H Watch rifketh herraint,\n",
    "Butigter by earth\n",
    "Oxmpold, a devil.\n",
    "\n",
    "----------------------------------------------\n",
    "```\n",
    "\n",
    "From trial2 model:\n",
    "\n",
    "```\n",
    " CAMILLO:\n",
    "Your marriage?\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    " CAMILLO:\n",
    "O me?\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    " CAMILLO:\n",
    "Go, sir, know it late not is cruel\n",
    "My humble that standtesy impaleful step thee well\n",
    "But well! in the heaven.\n",
    "\n",
    "----------------------------------------------\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
